<div align="center">

# ‚úàÔ∏è Turbofan Engine Predictive Maintenance AI

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue?logo=python&logoColor=white)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange?logo=tensorflow&logoColor=white)](https://www.tensorflow.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-Dashboard-red?logo=streamlit&logoColor=white)](https://streamlit.io/)
[![NASA Dataset](https://img.shields.io/badge/Dataset-NASA%20C--MAPSS-blue)](https://data.nasa.gov/dataset/cmapss-jet-engine-simulated-data)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**A deep learning system for predicting the Remaining Useful Life (RUL) of turbofan jet engines using LSTM networks, with a live interactive Streamlit dashboard.**

[Overview](#-overview) ‚Ä¢ [Dataset](#-dataset) ‚Ä¢ [Model](#-model-architecture) ‚Ä¢ [Dashboard](#-live-dashboard) ‚Ä¢ [Installation](#-installation) ‚Ä¢ [Usage](#-usage) ‚Ä¢ [Results](#-results) ‚Ä¢ [Team](#-team)

</div>

---

## üîç Overview

Unscheduled maintenance in the aerospace and heavy machinery industries is one of the costliest operational challenges. A single unexpected engine failure can ground an aircraft, delay hundreds of passengers, and cost millions of dollars.

This project addresses that challenge head-on by building a **Prognostics and Health Management (PHM)** system that:

- üìä **Ingests** raw time-series sensor data from turbofan engines
- üßπ **Engineers** meaningful features and removes noisy/constant sensors
- ü§ñ **Trains** a deep LSTM network to predict how many cycles remain before failure
- üìà **Visualises** predictions and sensor trends on a live Streamlit dashboard

> *"Predict failure before it happens ‚Äî not after."*

---

## üìÇ Project Structure

```
Turbofan-engine-predictive-maintenance-ai/
‚îú‚îÄ‚îÄ app.py                      # Streamlit dashboard
‚îú‚îÄ‚îÄ nasa_rul_model.h5           # Pre-trained LSTM model
‚îú‚îÄ‚îÄ train_FD001.txt             # Training set (FD001 subset)
‚îú‚îÄ‚îÄ test_FD001.txt              # Test set (FD001 subset)
‚îú‚îÄ‚îÄ RUL_FD001.txt               # Ground-truth RUL values for test set
‚îú‚îÄ‚îÄ RUL_NasaEngineFinal.ipynb   # Full analysis & training notebook
‚îú‚îÄ‚îÄ RUL_NasaEngineFinal.html    # Rendered HTML version of the notebook
‚îú‚îÄ‚îÄ RUL_NasaEngineFinal.pdf     # PDF version of the notebook
‚îî‚îÄ‚îÄ README.md
```

---

## üì° Dataset

**Source:** [NASA C-MAPSS Jet Engine Simulated Data](https://data.nasa.gov/dataset/cmapss-jet-engine-simulated-data)

The **Commercial Modular Aero-Propulsion System Simulation (C-MAPSS)** dataset was generated by NASA using a thermodynamic turbofan engine simulator. This project uses the **FD001** subset.

| Property | Value |
|---|---|
| Training engines | 100 units |
| Test engines | 100 units |
| Operating conditions | 1 |
| Fault modes | 1 (HPC degradation) |
| Sensors | 21 time-series measurements |
| Features used | 14 (after dropping constant/low-signal sensors) |

### Sensor Selection

After correlation analysis, the following low-information sensors were **dropped**:

`setting_1`, `setting_2`, `setting_3`, `s_1`, `s_5`, `s_6`, `s_10`, `s_14`, `s_16`, `s_18`, `s_19`

The **14 remaining sensors** (e.g. `s_2`, `s_3`, `s_4`, `s_7`, `s_11`, `s_12`) carry meaningful degradation signals and are used as model inputs.

### Target Variable ‚Äî RUL

```
RUL = max_cycle - current_cycle   (clipped at 125 cycles)
```

Clipping at 125 cycles reflects the real-world assumption that early-life behaviour is stable and unpredictable; only the degradation phase is modelled.

---

## üèóÔ∏è Model Architecture

### Preprocessing Pipeline

```
Raw sensor data
      ‚îÇ
      ‚ñº
Min-Max Normalization  ‚Üê fit on TRAIN only (no data leakage)
      ‚îÇ
      ‚ñº
Sliding Window  ‚Üí  sequences of shape (50 time steps √ó 14 features)
      ‚îÇ
      ‚ñº
LSTM Network
```

### LSTM Network

```
Input  ‚Üí  (batch, 50 time steps, 14 features)
      ‚îÇ
      ‚ñº  LSTM Layer 1 ‚Äî 128 units, tanh, return_sequences=True
      ‚ñº  Batch Normalisation
      ‚ñº  Dropout (0.2)
      ‚îÇ
      ‚ñº  LSTM Layer 2 ‚Äî 64 units, tanh
      ‚ñº  Dropout (0.2)
      ‚îÇ
      ‚ñº  Dense ‚Äî 32 units, ReLU
      ‚ñº  Dense ‚Äî 1 unit  (RUL regression output)
```

**Loss:** Mean Squared Error (MSE)  
**Optimizer:** Adam  
**Early Stopping:** patience = 10 epochs (monitors `val_loss`, restores best weights)

### Hyperparameter Tuning

An additional tuning pass was performed using **Keras Tuner (RandomSearch)** over:

| Hyperparameter | Search Range |
|---|---|
| LSTM units (layer 1) | 32 ‚Üí 128 (step 32) |
| LSTM units (layer 2) | 32 ‚Üí 64 (step 32) |
| Dropout rate | 0.1 ‚Üí 0.3 |
| Learning rate | 1e-4 ‚Üí 1e-2 |

The best configuration was re-trained for 50 epochs with the learning rate capped at `0.001` to avoid LSTM gradient instability.

---

## üìä Results

### Baseline LSTM

The baseline LSTM model (128 ‚Üí 64 stacked LSTM with Batch Normalisation and Dropout) was trained for up to 50 epochs with early stopping (patience = 10) and achieved the following on the 93 valid test units from **FD001**:

| Metric | Value |
|---|---|
| **RMSE** (cycles) | **13.68** |
| **MAE** (cycles) | **9.85** |
| **NASA S-Score** | **351.52** |

> RMSE of 13.68 cycles is below the commonly used 20-cycle threshold and is considered **highly effective** for this benchmark.

---

### Hyperparameter Tuning (Keras Tuner ‚Äî RandomSearch)

10 random trials were run. Best configuration found:

| Hyperparameter | Tuner Result | Applied Value |
|---|---|---|
| LSTM units (layer 1) | 128 | 128 |
| LSTM units (layer 2) | 64 | 64 |
| Dropout rate | 0.1 | 0.1 |
| Learning rate | 0.005 | 0.001 (capped for LSTM stability) |

Despite thorough tuning, the **Tuned LSTM underperformed** (RMSE 84.59) because the tuner only trains for 20 epochs per trial ‚Äî too short for LSTMs ‚Äî while the baseline was trained to convergence with early stopping. The Baseline LSTM was therefore retained as the production model.

---

### Multi-Model Comparison

Six deep learning architectures were trained and evaluated on the same FD001 test set:

| Rank | Model | RMSE ‚Üì | MAE ‚Üì | NASA S-Score ‚Üì | Parameters |
|:---:|---|---:|---:|---:|---:|
| ü•á | **Baseline LSTM** | **13.68** | **9.85** | 351.52 | 124,993 |
| ü•à | GRU | 13.78 | 10.60 | **320.97** | 95,041 |
| ü•â | TCN | 16.41 | 12.31 | 449.73 | 79,457 |
| 4th | CNN-LSTM | 16.44 | 12.31 | 613.40 | 37,985 |
| 5th | Transformer | 19.60 | 14.63 | 679.41 | 173,569 |
| 6th | Tuned LSTM | 84.59 | 75.81 | 1,446,608.82 | 94,657 |

**Metric definitions:**
- **RMSE** ‚Äî Root Mean Squared Error in engine cycles; primary metric, penalises large errors
- **MAE** ‚Äî Mean Absolute Error; average prediction offset in cycles
- **NASA S-Score** ‚Äî Asymmetric penalty metric; late predictions (underestimating RUL) are penalised more than early ones, reflecting real-world safety priorities. **Lower is better and safer.**

---

### üèÜ Winner & Inference

**Winner (best accuracy): Baseline LSTM ‚Äî RMSE 13.68 cycles, MAE 9.85 cycles**

The stacked 128 ‚Üí 64 LSTM with Batch Normalisation and Dropout(0.2) proved optimal. It achieved the lowest RMSE and MAE of all six architectures, meaning its predictions deviate from the true RUL by fewer than **10 cycles on average**.

**Safest model (lowest S-Score): GRU ‚Äî S-Score 320.97**

The GRU is only 0.10 cycles worse on RMSE (13.78 vs 13.68) but produces the safest prediction profile: it tends to predict slightly early (conservative), which is exactly what maintenance crews want ‚Äî warn before failure, not after. The GRU also trains ~10% faster with fewer parameters.

**Key inferences from the study:**

1. **RUL clipping at 125 cycles** was the single most impactful preprocessing step, reducing RMSE from 30+ to ~14 cycles. It reflects the domain truth that engines in early life behave stably with unpredictable failure onset, so only modelling the degradation phase makes sense.
2. **Simpler recurrent models (LSTM, GRU) beat complex architectures** on this dataset. With ~15,000 training sequences the Transformer and TCN did not have enough data to reach their full potential.
3. **Feature selection is critical** ‚Äî removing 11 dead / low-variance sensors reduced noise and improved convergence speed.
4. **Learning rate sensitivity** ‚Äî LSTM and GRU models required a learning rate ‚â§ 0.001; higher values caused gradient instability.
5. **Keras Tuner with short trials can mislead** ‚Äî the tuner identified the same architecture as the baseline but the truncated trial length (20 epochs) made the tuned model appear poor; always retrain the winner to convergence.

**Recommendation:**
- Deploy **Baseline LSTM** for highest accuracy.
- Use **GRU** as a fast-inference, safety-first alternative in resource-constrained environments.

---

**Evaluation plots generated by the notebook:**
- Training vs. Validation Loss curve (MSE & MAE per epoch)
- Actual RUL vs. Predicted RUL scatter plot (baseline model)
- Prediction overlay for all six models vs. actual RUL
- Bar charts comparing RMSE / MAE / S-Score across all models
- Sensor degradation trend per engine unit

---

## üñ•Ô∏è Live Dashboard

The Streamlit dashboard (`app.py`) lets you interactively inspect any engine unit from the test set.

**Features:**
- üéõÔ∏è Sidebar unit selector (Unit 1 ‚Äì 100)
- üî¢ Live RUL prediction metric (cycles remaining)
- üìâ Pressure sensor (s_11) trend chart
- üõ†Ô∏è Debugging panel showing data availability per unit

**Screenshot preview:**

> Run the app locally (see [Usage](#-usage)) and navigate to `http://localhost:8501`

---

## ‚öôÔ∏è Installation

### Prerequisites

- Python 3.8 or higher
- `pip` package manager

### Steps

```bash
# 1. Clone the repository
git clone https://github.com/95karthik4/Turbofan-engine-predictive-maintenance-ai.git
cd Turbofan-engine-predictive-maintenance-ai

# 2. (Recommended) Create a virtual environment
python -m venv venv
source venv/bin/activate        # Windows: venv\Scripts\activate

# 3. Install dependencies
pip install tensorflow pandas numpy scikit-learn matplotlib seaborn streamlit keras-tuner
```

---

## üöÄ Usage

### Run the Interactive Dashboard

```bash
streamlit run app.py
```

Open [http://localhost:8501](http://localhost:8501) in your browser. Use the sidebar to select an engine unit and see its predicted RUL and sensor trends instantly.

### Run the Full Notebook

```bash
jupyter notebook RUL_NasaEngineFinal.ipynb
```

Or view the pre-rendered versions:
- **HTML:** `RUL_NasaEngineFinal.html` ‚Äî open in any browser
- **PDF:** `RUL_NasaEngineFinal.pdf`

### Retrain the Model

Execute all cells in `RUL_NasaEngineFinal.ipynb` from top to bottom. The notebook will:
1. Load and clean the data
2. Build and train the baseline LSTM
3. Run Keras Tuner hyperparameter search
4. Train the optimised model
5. Evaluate and save `nasa_rul_model.h5`

---

## üõ†Ô∏è Tech Stack

| Tool | Purpose |
|---|---|
| **Python 3.8+** | Core language |
| **TensorFlow / Keras** | LSTM model training & inference |
| **Keras Tuner** | Automated hyperparameter optimisation |
| **Pandas / NumPy** | Data manipulation |
| **Scikit-learn** | Min-Max scaling, evaluation metrics |
| **Matplotlib / Seaborn** | Visualisation |
| **Streamlit** | Interactive web dashboard |

---

## üë• Team

**Group 5 ‚Äî Capstone Project**

| Name | Role |
|---|---|
| Karthik Kunnamkumarath | Project Lead |
| Aswin Anil Bindu | Data Engineering & Modelling |
| Sreelakshmi Nair | Feature Engineering & Analysis |
| Tuna G√ºzelmeri√ß | Hyperparameter Tuning |
| Cindy Mai | Dashboard & Evaluation |

---

## üìÑ License

This project is licensed under the **MIT License** ‚Äî see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgements

- **NASA** for the open-source [C-MAPSS dataset](https://data.nasa.gov/dataset/cmapss-jet-engine-simulated-data)
- The **PHM community** for benchmark methodologies and the NASA S-Score metric

---

<div align="center">
  <sub>Made with ‚ù§Ô∏è by Group 5</sub>
</div>
